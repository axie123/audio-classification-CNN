{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "cash_app_dessa_assignment.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N3m84rQqSF3u"
      },
      "source": [
        "### Introduction"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-uv6gaiqScIu"
      },
      "source": [
        "This tutorial will show how we can accurate classify audio sounds with deep learning."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LVxRl3JTSIXq"
      },
      "source": [
        "Note: my internet speeds seem to greatly affect how quickly the audio data processing and feature extraction with Librosa can be. Librosa's loading speeds vary drastically. The shortest time for loading was under 6 minutes and the longest was > 1 hour. Please keep note of this. Also, the GPU wasn't always available."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hWYMSTernepv"
      },
      "source": [
        "###Assumptions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b_u_IG8mhJ3O"
      },
      "source": [
        "\n",
        "1.   The audio data loaded are sufficently clean and distinct between labels that the feature extraction method used is sufficient for wave pattern distinction.\n",
        "2.   Librosa is the fastest audio file loader and feature extractor for this assignment. If librosa takes too long to load, you can use the preloaded **.npy** files in the shared folder using **np.load()** to the appropriate variables. For more information: https://numpy.org/doc/stable/reference/generated/numpy.load.html. The labels need to be one-hot encoded.\n",
        "3.   It is appropriate to represent the audio files in a spatial/spectral context given their nature to be train a CNN.\n",
        "4.   The GPU for Colab can be used for this project. If not, take out lines **with tf.device('/device:GPU:0'):** to get rid of the GPU.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J-la7tGbhN1G"
      },
      "source": [
        "### 1. Importing all the necessary modules and hardware."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zd_tMlFu_R2D"
      },
      "source": [
        "There are a 5 main modules that we would be using for this particular project.  \n",
        "\n",
        "1.   **os** - This is a Python built-in module that allows the engineer to access the local or cloud file systems.\n",
        "2.   **Librosa** - Python package typically used for music and audio analysis. We are also going to be using it for feature extraction.\n",
        "3.   **Numpy** - Scientific computing that will be used to process the raw audio files for deep learning.\n",
        "4.   **TensorFlow** - Deep learning framework that is going to be used to build and train the CNN. \n",
        "5.   **Matplotlib** - Useful for visualization during EDA. Please use a GPU for training.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "afSI64f2tazw"
      },
      "source": [
        "import os # We are going to need os to import the .wav files into the notebook to be processed.\n",
        "import numpy as np # This will be used to put the wave features of each .wav file into datasets.\n",
        "import seaborn as sn # This will be used for the confusion matrix.\n",
        "import pandas as pd # This will be used for the confusion matrix.\n",
        "import librosa # This is the audio file processing library that I have decided to use.\n",
        "from random import shuffle # This is used for shuffling the datasets once they are created.\n",
        "import matplotlib.pyplot as plt # This is used during the EDA.\n",
        "import tensorflow as tf # Deep learning framework used to develop the CNN for inference."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G47iT6QnOWFj"
      },
      "source": [
        "For this project, we want to use a GPU. This will help to drastically speed up training of the model. For this project, we can go to *Runtime -> Change runtime type -> Change to GPU runtime*. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zgy8YLpEOxnV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "26912c34-19f3-452e-d535-309c3ecb6a37"
      },
      "source": [
        "# This block of code is to check if the GPU instance is running on this notebook session.\n",
        "# We want to use a GPU to speed up data processing and CNN training time.\n",
        "\n",
        "# We want to see if the Google Drive gpu is available for use in this notebook session.\n",
        "# If it is available, the return on the notebook should say:\n",
        "# Found GPU at: /device:GPU:0\n",
        "\n",
        "device_name = tf.test.gpu_device_name() \n",
        "if device_name != '/device:GPU:0':\n",
        "    raise SystemError('GPU device not found')\n",
        "print('Found GPU at: {}'.format(device_name))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found GPU at: /device:GPU:0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5nsZndcXo07S"
      },
      "source": [
        "### 2. Mounting the Google Drive to access the audio data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "68Mqu2yRPAdI"
      },
      "source": [
        "For this project, we are going to use the dataset in **Dessa Cash Dataset.zip**. This contains 8732 **.wav** files. I had to unzip this file locally as I was unable to unzip the zip file directly on the Google Drive. Then, I uploaded the unzipped file onto the Google Drive. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uKlM-lXKTrUZ"
      },
      "source": [
        "In this case, I created a shortcut for the data folder I unzipped and shared in the assignment drive to my main Google Drive to access it."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eM6nHZdjhXZ3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "49b1d77d-018e-467c-f119-36ba292ca7c0"
      },
      "source": [
        "# This script would mount the drive onto the Colab Notebook.\n",
        "# If there is a drive mounted, the notebook would return:\n",
        "# \"Mounted at /content/drive\" or \n",
        "# \"Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\"\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ULXfILahygrL"
      },
      "source": [
        "AUDIO_DATASET = \"/content/drive/MyDrive/Dessa_Cash_Dataset\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k0v1WzAupkzB"
      },
      "source": [
        "###3. Exploratory Data Analysis "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CN6YRK0X2lQM"
      },
      "source": [
        "For EDA, we want to find out:\n",
        "\n",
        "1.   *The label distribution in the dataset* - It is important that our dataset is reasonably balanced and that no labels are over or underrepresented.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t1ErhSXJt2LA"
      },
      "source": [
        "# We want to make a histogram to look at the distribution of data among the label ids.\n",
        "labels = []"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yNHr7k5HSBrW"
      },
      "source": [
        "# We want to extract all the relative paths for the audio files into a list once.\n",
        "AUDIO_FILES = os.listdir(AUDIO_DATASET)\n",
        "AUDIO_DATA = [audio for audio in AUDIO_FILES if audio != '.DS_Store']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cEFtmD0Nm5cR"
      },
      "source": [
        "# Loading the array for the history:\n",
        "for sound in AUDIO_DATA:  \n",
        "    label = sound[-5]\n",
        "    labels.append(int(label))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 334
        },
        "id": "aI6QR0F1qYQB",
        "outputId": "a94ef03d-0225-4e5f-eefe-e76ce17014ed"
      },
      "source": [
        "# Plotting the histogram:\n",
        "\n",
        "bins = np.arange(10)-0.5\n",
        "plt.figure(figsize=(8,5))\n",
        "plt.hist(labels, bins=bins, color='green', rwidth=0.5)\n",
        "plt.xlabel('Label IDs')\n",
        "plt.ylabel('ID Frequency')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 576x360 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "onxX9sucrSY8"
      },
      "source": [
        "Based on the information shown in the histogram, the data is generally well balanced across all label IDs. The number of samples for labels 0, 2, 3, 4, 5, 7, and 8 are around the same. However, labels 1 and 6 are underrepresented with respect to the other labels by a ratio of 2-to-1. From this, we can expect the model to underperform relative to the other classes during validation and testing. Since there is no easy way to oversample these underrepresented classes, and undersampling will eliminate most of the dataset, we will accept these shortcomings and use the dataset the way it came."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SzWS2Zctw1GU"
      },
      "source": [
        "### 4. Training, Validation and Test Dataset Split"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sWe6_EzuV4JL"
      },
      "source": [
        "After we have done the EDA, we will now process process the audio in the .wav files and manipulate the data to prepare it for deep learning. Typically in a machine learning project, the training-validation-test split would be 80-10-10 in terms of percentages. To get this data split proportion, we extract every 9th .wav file for the validation set and every 10th .wav file for the test set within every class label. This would ensure that the distribution of the labels remain consistent throughout training, validation, and test datasets."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ud0yOBWxasd8"
      },
      "source": [
        "# We want to seperate the .wav files based on label id.\n",
        "label_ids = {\n",
        "     0: [],\n",
        "     1: [],\n",
        "     2: [],\n",
        "     3: [],\n",
        "     4: [], \n",
        "     5: [],\n",
        "     6: [],\n",
        "     7: [],\n",
        "     8: [],\n",
        "     9: []\n",
        "}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qS9_qK49bBK_"
      },
      "source": [
        "def populate_id_dict():\n",
        "    # This script would return a dictionary that would store the .wav file path based on their label id.\n",
        "    for sound in AUDIO_DATA:  \n",
        "        sound_path = os.path.join(AUDIO_DATASET, sound)\n",
        "        label = sound[-5]\n",
        "        label_ids[int(label)].append(sound_path)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ws3UZvPjjZb3"
      },
      "source": [
        "populate_id_dict()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b99qH6G6bxVv"
      },
      "source": [
        "# We want to split the audio dataset into a 80-10-10 training-validation-test split.\n",
        "\n",
        "# We want to store the file paths of the individual audio file paths for each of the 3 sub datasets.\n",
        "trainset_files = [] # This is for storing paths of the training set.\n",
        "validset_files = [] # This is for storing paths of the validation set.\n",
        "testset_files = [] # This is for storing paths of the test set.\n",
        "\n",
        "# We want to extract every 9th file for the validation set and every 10th for the test set\n",
        "# for each label id class.\n",
        "# This should distribute the data by it's class labels proportionally across all datasets.\n",
        "for label, files in label_ids.items():\n",
        "    for i, sound in enumerate(files):   \n",
        "        if (i+1) % 10 == 9:\n",
        "            validset_files.append(sound)\n",
        "        elif (i+1) % 10 == 0:\n",
        "            testset_files.append(sound)\n",
        "        else:\n",
        "            trainset_files.append(sound)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JfWBAJGOfEDn"
      },
      "source": [
        "# We want to shuffle the data paths for each dataset.\n",
        "# This would allow faster convergence during model training.\n",
        "shuffle(trainset_files)\n",
        "shuffle(validset_files)\n",
        "shuffle(testset_files)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "94G2Mi_Cifil"
      },
      "source": [
        "###5. Data Manipulation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sk9ksJ8qt3M2"
      },
      "source": [
        "The purpose of the data manipulation is to prepare it for deep learning.\n",
        "In this case, the Librosa library does a lot of the work extracting the amplitudes from the .wav files. We still need to set a number of features for the library to extract to be used for the model. I have decided to keep the final dataset simple and make it a 2-D array. Each row would represent single .wav file. This would also allow us to do batch SGD during training."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jBk5sWSSTGjX"
      },
      "source": [
        "# In accordance to standard approach, we are going to limit the number of feature measurements to 100.\n",
        "\n",
        "FEATURE_LEN = 100"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QknQNBa2n7IU"
      },
      "source": [
        "def extract_label_id(sound):\n",
        "    '''\n",
        "    This helper function is used to extract the label of the .wav file.\n",
        "\n",
        "    Input: the relative path of the .wav file in string type.\n",
        "    Output: an integer label of the audio file.\n",
        "    '''\n",
        "    label_id = sound[-5]\n",
        "    return int(label_id)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "axc_VGDTo1oY"
      },
      "source": [
        "def feature_extraction(sound_path):\n",
        "    '''\n",
        "    This helper function extracts the audio amplitudes and features from the .wav \n",
        "    file using Librosa. 'None' is passed in as the target_sample_rate since it \n",
        "    allows for fast loading and provides automatic normalization of the amplitudes. \n",
        "    One key detail that should be noted is that Librosa doesn't return arrays at a\n",
        "    consistent length. This issue will be addressed later on. \n",
        "\n",
        "    The featurs that we are extracting here are the audio's Mel-Frequency Cepstral \n",
        "    Coefficients (MFCCS). These are a spectral representation of an audio clip that\n",
        "    can be analyzed like an image.\n",
        "\n",
        "    Inputs: the relative path of the .wav file in string type, and the sampling rate.\n",
        "    Output: the audio file's MFCCS features in a numpy array form.\n",
        "    '''\n",
        "\n",
        "    # Audio Extraction.\n",
        "    audio, sample_rate = librosa.load(sound_path, sr=None) \n",
        "    # Feature Extraction.\n",
        "    mfccs = librosa.feature.mfcc(y=audio, sr=sample_rate, n_mfcc=100)\n",
        "    # Averging among the individua feature coefficient columns.\n",
        "    mfccs_avg = np.mean(mfccs,axis=1).reshape(-1, FEATURE_LEN)\n",
        "    return mfccs_avg"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yR9J4kU1pZ_0"
      },
      "source": [
        "def stack_and_concat_data(temp, audio_stack):\n",
        "    '''\n",
        "    This is a helper function that is used for the development of the 2-D array datasets.\n",
        "    It stacks the list of feature arrays into a 2D array that can be used for \n",
        "    deep learning.\n",
        "\n",
        "    input: temporary array for holding feature arrays and the final audio stack.\n",
        "    output: the final concatenated audio stack used for DL.\n",
        "    '''\n",
        "    array_stack = np.stack(temp,axis=0).reshape(-1,FEATURE_LEN)\n",
        "    audio_stack = np.concatenate([audio_stack, array_stack], axis=0)\n",
        "    return audio_stack"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kHnchD98EQmW"
      },
      "source": [
        "def concatenate_audio(audio_files):\n",
        "    '''\n",
        "    This function is meant to extract the audio from the .wav files and manipulate \n",
        "    them into a format that can be used for deep learning. This function has several\n",
        "    key components: label id extraction, amplitude and feature extraction, and \n",
        "    concatenation. We want to batch concatenate in feature array batches of 1000 at a time. \n",
        "    This is done mainly for computation speed. The speed of the Numpy concatenation method \n",
        "    doesn't differ that much between a few and 1000 dataframes, and it's much faster to \n",
        "    do them in bulk than one at a time.\n",
        "\n",
        "    input: relative paths for each of audio file datasets.\n",
        "    output: the DL datasets and their labels.\n",
        "    '''\n",
        "\n",
        "    audio_stack = np.array([])\n",
        "    audio_list = []\n",
        "    audio_label = []\n",
        "\n",
        "    # Using the GPU:\n",
        "    with tf.device('/device:GPU:0'):\n",
        "        # We have to iterate through all the .wav file paths to be processed.\n",
        "        for audio_file in audio_files:\n",
        "            # Gets the label ids of each .wav file.\n",
        "            label_id = extract_label_id(audio_file)\n",
        "            audio_label.append(label_id)\n",
        "            # Extracts the audio features from the .wav file.\n",
        "            processed_audio = feature_extraction(audio_file)\n",
        "            # We use this temporary list for storing every 1000 feature\n",
        "            # arrays for speed.\n",
        "            audio_list.append(processed_audio)\n",
        "            # We want to be able to initialize the final audio array stack dataset\n",
        "            # like this due to the nature of Numpy concatenate.\n",
        "            if len(audio_stack) == 0:\n",
        "                print(\"Initializing the data stack.....\")\n",
        "                audio_stack = processed_audio \n",
        "            # Every 1000 arrays in the temporary list, we would concatenate it\n",
        "            # to the main data stack and reset the temporary list.\n",
        "            elif len(audio_list) == 1000:\n",
        "                print(\"Adding to data stack.....\")\n",
        "                audio_stack = stack_and_concat_data(audio_list, audio_stack)\n",
        "            # We reset the temporary list of audio arrays.\n",
        "            audio_list = []\n",
        "        # If the temporary list doesn't have 1000 arrays, we just concatenate the\n",
        "        # remaining to the final dataset.\n",
        "        audio_stack = stack_and_concat_data(audio_list, audio_stack)\n",
        "        return audio_stack, audio_label"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YVlY-w2GESBO"
      },
      "source": [
        "# Creating the training, validation, and test datasets:\n",
        "training, train_label = concatenate_audio(trainset_files)\n",
        "validation, valid_label = concatenate_audio(validset_files)\n",
        "test, test_label = concatenate_audio(testset_files)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KbpB29OpALrX"
      },
      "source": [
        "# We want to reshape the training, validation, and test sets to fit the dimensions of the CNN.\n",
        "training, validation, test = training.reshape(len(trainset_files), FEATURE_LEN, 1), validation.reshape(len(validset_files), FEATURE_LEN, 1), test.reshape(len(testset_files), FEATURE_LEN, 1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jAuLvV6_S4rS"
      },
      "source": [
        "# We also want to one hot encode the labels for all 3 datasets.\n",
        "train_label = tf.keras.utils.to_categorical(train_label)\n",
        "valid_label = tf.keras.utils.to_categorical(valid_label)\n",
        "test_label = tf.keras.utils.to_categorical(test_label)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dlNZcWtfs-Jd"
      },
      "source": [
        "We also want to check the dimensions of all the datasets that we have created."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fedUH6cl1Sy2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ce24125c-10e9-43e7-a949-a7641afa25ea"
      },
      "source": [
        "print(training.shape)\n",
        "print(validation.shape)\n",
        "print(test.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(6988, 100, 1)\n",
            "(873, 100, 1)\n",
            "(871, 100, 1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xysWKSAe1VyP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "27c286cc-98c3-4a25-c6e9-0807280418a8"
      },
      "source": [
        "print(train_label.shape)\n",
        "print(valid_label.shape)\n",
        "print(test_label.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(6988, 10)\n",
            "(873, 10)\n",
            "(871, 10)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xIXU1iYWF_hc"
      },
      "source": [
        "### 6. Baseline Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w-mDVR4hO6sG"
      },
      "source": [
        "We want to start off with a baseline model. The baseline model should be simple to interpret and train. We have decided to go with a CNN. We chose a CNN because it is commonly used audio processing. A CNN also has the advantage of being able to capture the unique features of a particular class. In this case, it would be feature patterns at certain areas of the array."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "odiKyFkTIoBv"
      },
      "source": [
        "The baseline model will have 4 convolution layers. The number of filters are 128, 64, 64, and 32. We chose these numbers as it is common to be using filter numbers of 32, 64, 128 and so on initially as default. After each convolution layer, we add a size 4 1D max pooling layer. These max pooling layers are for capturing key amplitude features within the audio arrays for distinct features. Padding is added before each convolution layer if needed for the kernels. All parameters within the layers are Xavier Glorot initialized to prevent gradient explosion or vanishing. L2 Regularization of the standard 0.01 was added to prevent overfitting. A final connected layer was added after the convolution layers as a final processing layer before the softmax classification at the end."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mDx0-Xt98N0W"
      },
      "source": [
        "def baseline():\n",
        "    baseline = tf.keras.models.Sequential()\n",
        "    # First Convolution Layer.\n",
        "    baseline.add(tf.keras.layers.Conv1D(128,\n",
        "                 input_shape=[100,1],\n",
        "                 kernel_size=5,\n",
        "                 strides=2,\n",
        "                 padding='same',\n",
        "                 kernel_initializer='glorot_uniform'))\n",
        "    # Second Convolution Layer.\n",
        "    baseline.add(tf.keras.layers.Conv1D(64,\n",
        "                 kernel_size=4,\n",
        "                 strides=1,\n",
        "                 padding='same',\n",
        "                 kernel_initializer='glorot_uniform'))\n",
        "    # Third Convolution Layer.\n",
        "    baseline.add(tf.keras.layers.Conv1D(64,\n",
        "                 kernel_size=4,\n",
        "                 strides=1,\n",
        "                 padding='same',\n",
        "                 kernel_initializer='glorot_uniform'))\n",
        "    # Last Convolution Layer.\n",
        "    baseline.add(tf.keras.layers.Conv1D(32,\n",
        "                 kernel_size=3,\n",
        "                 strides=1,\n",
        "                 padding='same',\n",
        "                 kernel_initializer='glorot_uniform'))\n",
        "    baseline.add(tf.keras.layers.Reshape((1600,)))\n",
        "    # Final softmax layer for final classification over 10 classes.\n",
        "    baseline.add(tf.keras.layers.Dense(10, activation='softmax'))\n",
        "    return baseline"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HISllVCU9pct"
      },
      "source": [
        "batchsize = 128  # Batch size: the standard batch sizes are 64, 128, 256. We want to start somewhere in the middle so we can adjust.\n",
        "epochs = 30 # We can set the number of epochs to train for to be 30 for now.\n",
        "l_r = 1e-03  # Learning rate: the learning rate is usually logarithmically changed between 1e-07 to 0.1. 0.001 seems to be a good place to start.\n",
        "\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate = l_r) # Adam is computational efficient and requires little memory.\n",
        "ce_loss = tf.keras.losses.categorical_crossentropy # This is a good loss metric for multiple one-hot encoded classes.\n",
        "accuracy_metric = tf.keras.metrics.CategoricalAccuracy() # This is a good accuracy metric for multiple one-hot encoded classes."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QXntSnusF10L"
      },
      "source": [
        "baseline = baseline() # Compiling the model.\n",
        "baseline.compile(optimizer=optimizer, loss=ce_loss, metrics=[accuracy_metric])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ukVX4U9RQcWj",
        "outputId": "6c5859bc-040e-47a0-d6f0-4ecc3a4dda1b"
      },
      "source": [
        "# Checking the parameters of the model.\n",
        "baseline.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_7\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv1d_28 (Conv1D)           (None, 50, 128)           768       \n",
            "_________________________________________________________________\n",
            "conv1d_29 (Conv1D)           (None, 50, 64)            32832     \n",
            "_________________________________________________________________\n",
            "conv1d_30 (Conv1D)           (None, 50, 64)            16448     \n",
            "_________________________________________________________________\n",
            "conv1d_31 (Conv1D)           (None, 50, 32)            6176      \n",
            "_________________________________________________________________\n",
            "reshape_7 (Reshape)          (None, 1600)              0         \n",
            "_________________________________________________________________\n",
            "dense_7 (Dense)              (None, 10)                16010     \n",
            "=================================================================\n",
            "Total params: 72,234\n",
            "Trainable params: 72,234\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Av0yy1mDeKEE"
      },
      "source": [
        "from keras.callbacks import ReduceLROnPlateau # This is used to adjust learning rate."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hyuMUx649rug",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "011bdd96-d262-4905-9f6e-343ef1a4c1dc"
      },
      "source": [
        "# This is a callback that reduces the learning rate by a factor of half if the validation accuracy doesn't increase within 5 epochs.\n",
        "reduce_lr = ReduceLROnPlateau(monitor='val_categorical_accuracy', factor=0.5, patience=5, min_lr=0.0000001, verbose=1)\n",
        "\n",
        "# Commences training of the model.\n",
        "with tf.device('/device:GPU:0'):\n",
        "    history = baseline.fit(x=training, y=train_label, batch_size=batchsize, epochs=epochs,\n",
        "                           validation_data=(validation, valid_label), \n",
        "                           shuffle=True, initial_epoch=0, callbacks=[reduce_lr])\n",
        "\n",
        "# We want a place to store the training and validation loss and accuracy.\n",
        "training_loss = history.history['loss']\n",
        "training_accuracy = history.history['categorical_accuracy']\n",
        "valid_loss = history.history['val_loss']\n",
        "valid_accuracy = history.history['val_categorical_accuracy']"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/30\n",
            "55/55 [==============================] - 6s 96ms/step - loss: 3.0347 - categorical_accuracy: 0.3134 - val_loss: 1.2410 - val_categorical_accuracy: 0.5991\n",
            "Epoch 2/30\n",
            "55/55 [==============================] - 5s 89ms/step - loss: 1.1784 - categorical_accuracy: 0.6167 - val_loss: 1.1793 - val_categorical_accuracy: 0.6289\n",
            "Epoch 3/30\n",
            "55/55 [==============================] - 5s 90ms/step - loss: 1.0929 - categorical_accuracy: 0.6486 - val_loss: 1.1532 - val_categorical_accuracy: 0.6300\n",
            "Epoch 4/30\n",
            "55/55 [==============================] - 5s 87ms/step - loss: 1.0767 - categorical_accuracy: 0.6494 - val_loss: 1.1184 - val_categorical_accuracy: 0.6529\n",
            "Epoch 5/30\n",
            "55/55 [==============================] - 5s 87ms/step - loss: 1.0272 - categorical_accuracy: 0.6690 - val_loss: 1.1422 - val_categorical_accuracy: 0.6174\n",
            "Epoch 6/30\n",
            "55/55 [==============================] - 5s 88ms/step - loss: 1.0172 - categorical_accuracy: 0.6731 - val_loss: 1.1305 - val_categorical_accuracy: 0.6289\n",
            "Epoch 7/30\n",
            "55/55 [==============================] - 5s 89ms/step - loss: 1.0035 - categorical_accuracy: 0.6731 - val_loss: 1.1437 - val_categorical_accuracy: 0.6277\n",
            "Epoch 8/30\n",
            "55/55 [==============================] - 5s 88ms/step - loss: 1.0343 - categorical_accuracy: 0.6591 - val_loss: 1.1143 - val_categorical_accuracy: 0.6506\n",
            "Epoch 9/30\n",
            "55/55 [==============================] - 5s 92ms/step - loss: 0.9943 - categorical_accuracy: 0.6822 - val_loss: 1.0706 - val_categorical_accuracy: 0.6632\n",
            "Epoch 10/30\n",
            "55/55 [==============================] - 5s 92ms/step - loss: 0.9752 - categorical_accuracy: 0.6859 - val_loss: 1.1133 - val_categorical_accuracy: 0.6403\n",
            "Epoch 11/30\n",
            "55/55 [==============================] - 5s 93ms/step - loss: 0.9981 - categorical_accuracy: 0.6852 - val_loss: 1.0739 - val_categorical_accuracy: 0.6678\n",
            "Epoch 12/30\n",
            "55/55 [==============================] - 5s 92ms/step - loss: 0.9476 - categorical_accuracy: 0.6962 - val_loss: 1.0887 - val_categorical_accuracy: 0.6529\n",
            "Epoch 13/30\n",
            "55/55 [==============================] - 5s 97ms/step - loss: 0.9642 - categorical_accuracy: 0.6919 - val_loss: 1.0823 - val_categorical_accuracy: 0.6552\n",
            "Epoch 14/30\n",
            "55/55 [==============================] - 5s 95ms/step - loss: 0.9680 - categorical_accuracy: 0.6830 - val_loss: 1.0925 - val_categorical_accuracy: 0.6518\n",
            "Epoch 15/30\n",
            "55/55 [==============================] - 5s 97ms/step - loss: 1.0110 - categorical_accuracy: 0.6712 - val_loss: 1.1058 - val_categorical_accuracy: 0.6472\n",
            "Epoch 16/30\n",
            "55/55 [==============================] - 5s 91ms/step - loss: 0.9446 - categorical_accuracy: 0.6973 - val_loss: 1.0629 - val_categorical_accuracy: 0.6632\n",
            "\n",
            "Epoch 00016: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
            "Epoch 17/30\n",
            "55/55 [==============================] - 5s 91ms/step - loss: 0.9095 - categorical_accuracy: 0.7019 - val_loss: 1.0438 - val_categorical_accuracy: 0.6735\n",
            "Epoch 18/30\n",
            "55/55 [==============================] - 5s 91ms/step - loss: 0.9093 - categorical_accuracy: 0.7148 - val_loss: 1.0434 - val_categorical_accuracy: 0.6793\n",
            "Epoch 19/30\n",
            "55/55 [==============================] - 5s 92ms/step - loss: 0.8976 - categorical_accuracy: 0.7125 - val_loss: 1.0561 - val_categorical_accuracy: 0.6735\n",
            "Epoch 20/30\n",
            "55/55 [==============================] - 5s 93ms/step - loss: 0.9275 - categorical_accuracy: 0.7006 - val_loss: 1.0491 - val_categorical_accuracy: 0.6712\n",
            "Epoch 21/30\n",
            "55/55 [==============================] - 5s 95ms/step - loss: 0.9208 - categorical_accuracy: 0.7065 - val_loss: 1.0879 - val_categorical_accuracy: 0.6438\n",
            "Epoch 22/30\n",
            "55/55 [==============================] - 5s 87ms/step - loss: 0.9328 - categorical_accuracy: 0.7067 - val_loss: 1.1013 - val_categorical_accuracy: 0.6300\n",
            "Epoch 23/30\n",
            "55/55 [==============================] - 5s 88ms/step - loss: 0.9393 - categorical_accuracy: 0.7034 - val_loss: 1.0569 - val_categorical_accuracy: 0.6564\n",
            "\n",
            "Epoch 00023: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
            "Epoch 24/30\n",
            "55/55 [==============================] - 5s 85ms/step - loss: 0.9004 - categorical_accuracy: 0.7151 - val_loss: 1.0346 - val_categorical_accuracy: 0.6861\n",
            "Epoch 25/30\n",
            "55/55 [==============================] - 5s 84ms/step - loss: 0.8991 - categorical_accuracy: 0.7124 - val_loss: 1.0372 - val_categorical_accuracy: 0.6770\n",
            "Epoch 26/30\n",
            "55/55 [==============================] - 5s 85ms/step - loss: 0.8815 - categorical_accuracy: 0.7239 - val_loss: 1.0465 - val_categorical_accuracy: 0.6678\n",
            "Epoch 27/30\n",
            "55/55 [==============================] - 5s 87ms/step - loss: 0.8929 - categorical_accuracy: 0.7117 - val_loss: 1.0591 - val_categorical_accuracy: 0.6644\n",
            "Epoch 28/30\n",
            "55/55 [==============================] - 5s 86ms/step - loss: 0.9220 - categorical_accuracy: 0.7064 - val_loss: 1.0462 - val_categorical_accuracy: 0.6781\n",
            "Epoch 29/30\n",
            "55/55 [==============================] - 5s 89ms/step - loss: 0.9007 - categorical_accuracy: 0.7182 - val_loss: 1.0438 - val_categorical_accuracy: 0.6781\n",
            "\n",
            "Epoch 00029: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
            "Epoch 30/30\n",
            "55/55 [==============================] - 5s 87ms/step - loss: 0.8789 - categorical_accuracy: 0.7252 - val_loss: 1.0378 - val_categorical_accuracy: 0.6804\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MiAJv8WeXI25"
      },
      "source": [
        "We seem to have a proper baseline model for experimentation at this point. The training accuracy seems to have reached a relatively high accuracy within 30 epochs, finalizing at 72.98%. However, the validation accuracy has plateaued between 63% to 68% early on at around 5 epochs. Thie clearly shows that the model is overfitting. We will discuss how to mitigate these problems in the next section. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3iDjFH2IX-pG"
      },
      "source": [
        "### 7. Improvement Models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "37Lz_sembLQM"
      },
      "source": [
        "To mitigate overfitting, we are going to apply L2 regularization on  each convolution layer and ReLu activation after each convolution layer to help reduce overfitting. This should allow the validation accuracy to progress and keep up with the training accuracy."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "--KqqjO3YC_y"
      },
      "source": [
        "def model1():\n",
        "    model1 = tf.keras.models.Sequential()\n",
        "    # First Convolution Layer.\n",
        "    model1.add(tf.keras.layers.Conv1D(128,\n",
        "                 input_shape=[100,1],\n",
        "                 kernel_size=5,\n",
        "                 strides=2,\n",
        "                 padding='same',\n",
        "                 kernel_initializer='glorot_uniform',\n",
        "                 kernel_regularizer=tf.keras.regularizers.l2(l=0.01)))\n",
        "    # ReLu activation added.\n",
        "    model1.add(tf.keras.layers.Activation('relu'))\n",
        "    # Second Convolution Layer.\n",
        "    model1.add(tf.keras.layers.Conv1D(64,\n",
        "                 kernel_size=4,\n",
        "                 strides=1,\n",
        "                 padding='same',\n",
        "                 kernel_initializer='glorot_uniform',\n",
        "                 kernel_regularizer=tf.keras.regularizers.l2(l=0.01)))\n",
        "    # ReLu activation added.\n",
        "    model1.add(tf.keras.layers.Activation('relu'))\n",
        "    # Third Convolution Layer.\n",
        "    model1.add(tf.keras.layers.Conv1D(64,\n",
        "                 kernel_size=4,\n",
        "                 strides=1,\n",
        "                 padding='same',\n",
        "                 kernel_initializer='glorot_uniform',\n",
        "                 kernel_regularizer=tf.keras.regularizers.l2(l=0.01)))\n",
        "    # ReLu activation added.\n",
        "    model1.add(tf.keras.layers.Activation('relu'))\n",
        "    # Last Convolution Layer.\n",
        "    model1.add(tf.keras.layers.Conv1D(32,\n",
        "                 kernel_size=3,\n",
        "                 strides=1,\n",
        "                 padding='same',\n",
        "                 kernel_initializer='glorot_uniform',\n",
        "                 kernel_regularizer=tf.keras.regularizers.l2(l=0.01)))\n",
        "    # ReLu activation added.\n",
        "    model1.add(tf.keras.layers.Activation('relu'))\n",
        "    model1.add(tf.keras.layers.Reshape((1600,)))\n",
        "    # Final softmax layer for final classification over 10 classes.\n",
        "    model1.add(tf.keras.layers.Dense(10, activation='softmax'))\n",
        "    return model1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O_vupZd5qp6l"
      },
      "source": [
        "# We need to reset the optimizer, loss function, and accuracy metric for each model.\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate = l_r) # Adam is computational efficient and requires little memory.\n",
        "ce_loss = tf.keras.losses.categorical_crossentropy # This is a good loss metric for multiple one-hot encoded classes.\n",
        "accuracy_metric = tf.keras.metrics.CategoricalAccuracy() # This is a good accuracy metric for multiple one-hot encoded classes."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r2CVQ_TLYz6h"
      },
      "source": [
        "model1 = model1() # Compiling the model.\n",
        "model1.compile(optimizer=optimizer, loss=ce_loss, metrics=[accuracy_metric])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XK18lUgO0Xw8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "abbb8545-3771-4c06-bc3f-26bfd2ae7196"
      },
      "source": [
        "model1.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_8\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv1d_32 (Conv1D)           (None, 50, 128)           768       \n",
            "_________________________________________________________________\n",
            "activation_8 (Activation)    (None, 50, 128)           0         \n",
            "_________________________________________________________________\n",
            "conv1d_33 (Conv1D)           (None, 50, 64)            32832     \n",
            "_________________________________________________________________\n",
            "activation_9 (Activation)    (None, 50, 64)            0         \n",
            "_________________________________________________________________\n",
            "conv1d_34 (Conv1D)           (None, 50, 64)            16448     \n",
            "_________________________________________________________________\n",
            "activation_10 (Activation)   (None, 50, 64)            0         \n",
            "_________________________________________________________________\n",
            "conv1d_35 (Conv1D)           (None, 50, 32)            6176      \n",
            "_________________________________________________________________\n",
            "activation_11 (Activation)   (None, 50, 32)            0         \n",
            "_________________________________________________________________\n",
            "reshape_8 (Reshape)          (None, 1600)              0         \n",
            "_________________________________________________________________\n",
            "dense_8 (Dense)              (None, 10)                16010     \n",
            "=================================================================\n",
            "Total params: 72,234\n",
            "Trainable params: 72,234\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eCt6FTP1ZIM3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d40509f3-ac49-4421-98e6-fc96875eb214"
      },
      "source": [
        "# This is a callback that reduces the learning rate by a factor of half if the validation accuracy doesn't increase within 5 epochs.\n",
        "reduce_lr = ReduceLROnPlateau(monitor='val_categorical_accuracy', factor=0.5, patience=5, min_lr=0.0000001, verbose=1)\n",
        "\n",
        "# Commences training of the model.\n",
        "with tf.device('/device:GPU:0'):\n",
        "    history_2 = model1.fit(x=training, y=train_label, batch_size=batchsize, epochs=epochs,\n",
        "                           validation_data=(validation, valid_label), \n",
        "                           shuffle=True, initial_epoch=0, callbacks=[reduce_lr])\n",
        "\n",
        "# We want a place to store the training and validation loss and accuracy.\n",
        "training_loss = history_2.history['loss']\n",
        "training_accuracy = history_2.history['categorical_accuracy']\n",
        "valid_loss = history_2.history['val_loss']\n",
        "valid_accuracy = history_2.history['val_categorical_accuracy']"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/30\n",
            "55/55 [==============================] - 6s 95ms/step - loss: 3.6214 - categorical_accuracy: 0.3539 - val_loss: 2.3327 - val_categorical_accuracy: 0.6312\n",
            "Epoch 2/30\n",
            "55/55 [==============================] - 5s 91ms/step - loss: 2.1012 - categorical_accuracy: 0.6832 - val_loss: 1.8262 - val_categorical_accuracy: 0.7068\n",
            "Epoch 3/30\n",
            "55/55 [==============================] - 5s 89ms/step - loss: 1.6035 - categorical_accuracy: 0.7653 - val_loss: 1.5174 - val_categorical_accuracy: 0.7617\n",
            "Epoch 4/30\n",
            "55/55 [==============================] - 5s 90ms/step - loss: 1.3543 - categorical_accuracy: 0.8116 - val_loss: 1.3270 - val_categorical_accuracy: 0.8064\n",
            "Epoch 5/30\n",
            "55/55 [==============================] - 5s 91ms/step - loss: 1.1843 - categorical_accuracy: 0.8355 - val_loss: 1.1812 - val_categorical_accuracy: 0.8202\n",
            "Epoch 6/30\n",
            "55/55 [==============================] - 5s 88ms/step - loss: 1.0388 - categorical_accuracy: 0.8604 - val_loss: 1.1508 - val_categorical_accuracy: 0.8144\n",
            "Epoch 7/30\n",
            "55/55 [==============================] - 5s 88ms/step - loss: 0.9526 - categorical_accuracy: 0.8757 - val_loss: 1.0656 - val_categorical_accuracy: 0.8351\n",
            "Epoch 8/30\n",
            "55/55 [==============================] - 5s 89ms/step - loss: 0.8847 - categorical_accuracy: 0.8883 - val_loss: 0.9865 - val_categorical_accuracy: 0.8568\n",
            "Epoch 9/30\n",
            "55/55 [==============================] - 5s 89ms/step - loss: 0.8207 - categorical_accuracy: 0.8879 - val_loss: 0.9458 - val_categorical_accuracy: 0.8488\n",
            "Epoch 10/30\n",
            "55/55 [==============================] - 5s 91ms/step - loss: 0.7534 - categorical_accuracy: 0.9115 - val_loss: 0.9132 - val_categorical_accuracy: 0.8557\n",
            "Epoch 11/30\n",
            "55/55 [==============================] - 5s 92ms/step - loss: 0.7048 - categorical_accuracy: 0.9155 - val_loss: 0.9145 - val_categorical_accuracy: 0.8580\n",
            "Epoch 12/30\n",
            "55/55 [==============================] - 5s 91ms/step - loss: 0.6744 - categorical_accuracy: 0.9173 - val_loss: 0.8381 - val_categorical_accuracy: 0.8706\n",
            "Epoch 13/30\n",
            "55/55 [==============================] - 5s 92ms/step - loss: 0.6432 - categorical_accuracy: 0.9300 - val_loss: 0.8199 - val_categorical_accuracy: 0.8625\n",
            "Epoch 14/30\n",
            "55/55 [==============================] - 5s 96ms/step - loss: 0.6166 - categorical_accuracy: 0.9330 - val_loss: 0.8149 - val_categorical_accuracy: 0.8740\n",
            "Epoch 15/30\n",
            "55/55 [==============================] - 5s 94ms/step - loss: 0.6068 - categorical_accuracy: 0.9297 - val_loss: 0.7974 - val_categorical_accuracy: 0.8751\n",
            "Epoch 16/30\n",
            "55/55 [==============================] - 5s 95ms/step - loss: 0.5696 - categorical_accuracy: 0.9414 - val_loss: 0.7791 - val_categorical_accuracy: 0.8774\n",
            "Epoch 17/30\n",
            "55/55 [==============================] - 5s 95ms/step - loss: 0.5507 - categorical_accuracy: 0.9369 - val_loss: 0.8102 - val_categorical_accuracy: 0.8625\n",
            "Epoch 18/30\n",
            "55/55 [==============================] - 5s 96ms/step - loss: 0.5437 - categorical_accuracy: 0.9370 - val_loss: 0.7792 - val_categorical_accuracy: 0.8614\n",
            "Epoch 19/30\n",
            "55/55 [==============================] - 5s 94ms/step - loss: 0.5214 - categorical_accuracy: 0.9442 - val_loss: 0.7963 - val_categorical_accuracy: 0.8614\n",
            "Epoch 20/30\n",
            "55/55 [==============================] - 5s 94ms/step - loss: 0.5054 - categorical_accuracy: 0.9426 - val_loss: 0.7249 - val_categorical_accuracy: 0.8832\n",
            "Epoch 21/30\n",
            "55/55 [==============================] - 5s 93ms/step - loss: 0.4806 - categorical_accuracy: 0.9534 - val_loss: 0.7113 - val_categorical_accuracy: 0.8877\n",
            "Epoch 22/30\n",
            "55/55 [==============================] - 5s 98ms/step - loss: 0.4608 - categorical_accuracy: 0.9588 - val_loss: 0.7016 - val_categorical_accuracy: 0.8832\n",
            "Epoch 23/30\n",
            "55/55 [==============================] - 5s 94ms/step - loss: 0.4642 - categorical_accuracy: 0.9540 - val_loss: 0.6918 - val_categorical_accuracy: 0.8820\n",
            "Epoch 24/30\n",
            "55/55 [==============================] - 5s 94ms/step - loss: 0.4453 - categorical_accuracy: 0.9549 - val_loss: 0.6830 - val_categorical_accuracy: 0.9003\n",
            "Epoch 25/30\n",
            "55/55 [==============================] - 5s 97ms/step - loss: 0.4241 - categorical_accuracy: 0.9637 - val_loss: 0.6953 - val_categorical_accuracy: 0.8855\n",
            "Epoch 26/30\n",
            "55/55 [==============================] - 5s 96ms/step - loss: 0.4208 - categorical_accuracy: 0.9660 - val_loss: 0.6833 - val_categorical_accuracy: 0.8866\n",
            "Epoch 27/30\n",
            "55/55 [==============================] - 5s 94ms/step - loss: 0.4190 - categorical_accuracy: 0.9631 - val_loss: 0.6930 - val_categorical_accuracy: 0.9003\n",
            "Epoch 28/30\n",
            "55/55 [==============================] - 5s 93ms/step - loss: 0.4070 - categorical_accuracy: 0.9666 - val_loss: 0.6580 - val_categorical_accuracy: 0.8877\n",
            "Epoch 29/30\n",
            "55/55 [==============================] - 5s 94ms/step - loss: 0.4063 - categorical_accuracy: 0.9645 - val_loss: 0.6945 - val_categorical_accuracy: 0.8740\n",
            "\n",
            "Epoch 00029: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
            "Epoch 30/30\n",
            "55/55 [==============================] - 5s 95ms/step - loss: 0.3824 - categorical_accuracy: 0.9750 - val_loss: 0.6351 - val_categorical_accuracy: 0.8992\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JZvgTSYTELWz"
      },
      "source": [
        "This improved version of the neural network was able to deliver much faster converge and significantly better results in both training and validation accuracy, reaching 97.50% and 89.92% respectively. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WcyJDitLeNbl"
      },
      "source": [
        "We hope to increase the performance of the neural network by adding a 1D max pooling layer between each convolution layer and a fully connected layer after the convolutions. This theoretically should allow the model to reduce noise and dimensionality of the data in between convolutions and make finding distinctive patterns easier."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ri2BWJUDvVQx"
      },
      "source": [
        "def model2():\n",
        "    model2 = tf.keras.models.Sequential()\n",
        "    # First Convolution Layer.\n",
        "    model2.add(tf.keras.layers.Conv1D(128,\n",
        "                 input_shape=[100,1],\n",
        "                 kernel_size=5,\n",
        "                 strides=2,\n",
        "                 padding='same',\n",
        "                 kernel_initializer='glorot_uniform',\n",
        "                 kernel_regularizer=tf.keras.regularizers.l2(l=0.01)))\n",
        "    model2.add(tf.keras.layers.Activation('relu'))\n",
        "    # Adding a max pooling layer to reduce noise and dimensionality.\n",
        "    model2.add(tf.keras.layers.MaxPooling1D(pool_size=2, strides=1))\n",
        "    # Second Convolution Layer.\n",
        "    model2.add(tf.keras.layers.Conv1D(64,\n",
        "                 kernel_size=4,\n",
        "                 strides=1,\n",
        "                 padding='same',\n",
        "                 kernel_initializer='glorot_uniform',\n",
        "                 kernel_regularizer=tf.keras.regularizers.l2(l=0.01)))\n",
        "    model2.add(tf.keras.layers.Activation('relu'))\n",
        "    # Adding a max pooling layer to reduce noise and dimensionality.\n",
        "    model2.add(tf.keras.layers.MaxPooling1D(pool_size=2, strides=1))\n",
        "    # Third Convolution Layer.\n",
        "    model2.add(tf.keras.layers.Conv1D(64,\n",
        "                 kernel_size=4,\n",
        "                 strides=1,\n",
        "                 padding='same',\n",
        "                 kernel_initializer='glorot_uniform',\n",
        "                 kernel_regularizer=tf.keras.regularizers.l2(l=0.01)))\n",
        "    model2.add(tf.keras.layers.Activation('relu'))\n",
        "    # Adding a max pooling layer to reduce noise and dimensionality.\n",
        "    model2.add(tf.keras.layers.MaxPooling1D(pool_size=2, strides=1))\n",
        "    # Last Convolution Layer.\n",
        "    model2.add(tf.keras.layers.Conv1D(32,\n",
        "                 kernel_size=3,\n",
        "                 strides=1,\n",
        "                 padding='same',\n",
        "                 kernel_initializer='glorot_uniform',\n",
        "                 kernel_regularizer=tf.keras.regularizers.l2(l=0.01)))\n",
        "    model2.add(tf.keras.layers.Activation('relu'))\n",
        "    # Adding a max pooling layer to reduce noise and dimensionality.\n",
        "    model2.add(tf.keras.layers.MaxPooling1D(pool_size=2, strides=1))\n",
        "    model2.add(tf.keras.layers.Reshape((1472,)))\n",
        "    # Adding a fully connected layer to help with pattern recognition.\n",
        "    model2.add(tf.keras.layers.Dense(1000, activation='relu', \n",
        "                                     kernel_initializer='glorot_uniform',\n",
        "                                     bias_initializer='zeros'))\n",
        "    model2.add(tf.keras.layers.Dropout(0.2))\n",
        "    # Final softmax layer for final classification over 10 classes.\n",
        "    model2.add(tf.keras.layers.Dense(10, activation='softmax'))\n",
        "    return model2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UtaKghInwv5c"
      },
      "source": [
        "# We need to reset the optimizer, loss function, and accuracy metric for each model.\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate = l_r) # Adam is computational efficient and requires little memory.\n",
        "ce_loss = tf.keras.losses.categorical_crossentropy # This is a good loss metric for multiple one-hot encoded classes.\n",
        "accuracy_metric = tf.keras.metrics.CategoricalAccuracy() # This is a good accuracy metric for multiple one-hot encoded classes."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JylJuud2wyEM"
      },
      "source": [
        "model2 = model2() # Compiling the model.\n",
        "model2.compile(optimizer=optimizer, loss=ce_loss, metrics=[accuracy_metric])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R-K2ILWe0cpO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "24389fb3-39e5-4c8c-d155-7ba28299f846"
      },
      "source": [
        "model2.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_11\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv1d_44 (Conv1D)           (None, 50, 128)           768       \n",
            "_________________________________________________________________\n",
            "activation_20 (Activation)   (None, 50, 128)           0         \n",
            "_________________________________________________________________\n",
            "max_pooling1d_12 (MaxPooling (None, 49, 128)           0         \n",
            "_________________________________________________________________\n",
            "conv1d_45 (Conv1D)           (None, 49, 64)            32832     \n",
            "_________________________________________________________________\n",
            "activation_21 (Activation)   (None, 49, 64)            0         \n",
            "_________________________________________________________________\n",
            "max_pooling1d_13 (MaxPooling (None, 48, 64)            0         \n",
            "_________________________________________________________________\n",
            "conv1d_46 (Conv1D)           (None, 48, 64)            16448     \n",
            "_________________________________________________________________\n",
            "activation_22 (Activation)   (None, 48, 64)            0         \n",
            "_________________________________________________________________\n",
            "max_pooling1d_14 (MaxPooling (None, 47, 64)            0         \n",
            "_________________________________________________________________\n",
            "conv1d_47 (Conv1D)           (None, 47, 32)            6176      \n",
            "_________________________________________________________________\n",
            "activation_23 (Activation)   (None, 47, 32)            0         \n",
            "_________________________________________________________________\n",
            "max_pooling1d_15 (MaxPooling (None, 46, 32)            0         \n",
            "_________________________________________________________________\n",
            "reshape_11 (Reshape)         (None, 1472)              0         \n",
            "_________________________________________________________________\n",
            "dense_11 (Dense)             (None, 1000)              1473000   \n",
            "_________________________________________________________________\n",
            "dropout_1 (Dropout)          (None, 1000)              0         \n",
            "_________________________________________________________________\n",
            "dense_12 (Dense)             (None, 10)                10010     \n",
            "=================================================================\n",
            "Total params: 1,539,234\n",
            "Trainable params: 1,539,234\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "32OrosY9w4lJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6d1ffa3f-0e4f-409b-ac31-214f8a054e85"
      },
      "source": [
        "# This is a callback that reduces the learning rate by a factor of half if the validation accuracy doesn't increase within 5 epochs.\n",
        "reduce_lr = ReduceLROnPlateau(monitor='val_categorical_accuracy', factor=0.5, patience=5, min_lr=0.0000001, verbose=1)\n",
        "\n",
        "# Commences training of the model.\n",
        "with tf.device('/device:GPU:0'):\n",
        "    history_3 = model2.fit(x=training, y=train_label, batch_size=batchsize, epochs=epochs,\n",
        "                           validation_data=(validation, valid_label), \n",
        "                           shuffle=True, initial_epoch=0, callbacks=[reduce_lr])\n",
        "\n",
        "# We want a place to store the training and validation loss and accuracy.\n",
        "training_loss = history_3.history['loss']\n",
        "training_accuracy = history_3.history['categorical_accuracy']\n",
        "valid_loss = history_3.history['val_loss']\n",
        "valid_accuracy = history_3.history['val_categorical_accuracy']"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/30\n",
            "55/55 [==============================] - 9s 145ms/step - loss: 3.8598 - categorical_accuracy: 0.3222 - val_loss: 2.4140 - val_categorical_accuracy: 0.6483\n",
            "Epoch 2/30\n",
            "55/55 [==============================] - 10s 177ms/step - loss: 2.2151 - categorical_accuracy: 0.6999 - val_loss: 1.9353 - val_categorical_accuracy: 0.7090\n",
            "Epoch 3/30\n",
            "55/55 [==============================] - 8s 148ms/step - loss: 1.6945 - categorical_accuracy: 0.7761 - val_loss: 1.4973 - val_categorical_accuracy: 0.7995\n",
            "Epoch 4/30\n",
            "55/55 [==============================] - 8s 137ms/step - loss: 1.3272 - categorical_accuracy: 0.8453 - val_loss: 1.3173 - val_categorical_accuracy: 0.8190\n",
            "Epoch 5/30\n",
            "55/55 [==============================] - 7s 136ms/step - loss: 1.0994 - categorical_accuracy: 0.8850 - val_loss: 1.1112 - val_categorical_accuracy: 0.8683\n",
            "Epoch 6/30\n",
            "55/55 [==============================] - 7s 136ms/step - loss: 0.9119 - categorical_accuracy: 0.9174 - val_loss: 1.0080 - val_categorical_accuracy: 0.8660\n",
            "Epoch 7/30\n",
            "55/55 [==============================] - 7s 136ms/step - loss: 0.7903 - categorical_accuracy: 0.9280 - val_loss: 0.9625 - val_categorical_accuracy: 0.8797\n",
            "Epoch 8/30\n",
            "55/55 [==============================] - 8s 142ms/step - loss: 0.7038 - categorical_accuracy: 0.9369 - val_loss: 0.9046 - val_categorical_accuracy: 0.8637\n",
            "Epoch 9/30\n",
            "55/55 [==============================] - 8s 140ms/step - loss: 0.6320 - categorical_accuracy: 0.9462 - val_loss: 0.7880 - val_categorical_accuracy: 0.8923\n",
            "Epoch 10/30\n",
            "55/55 [==============================] - 8s 137ms/step - loss: 0.5417 - categorical_accuracy: 0.9656 - val_loss: 0.7278 - val_categorical_accuracy: 0.8992\n",
            "Epoch 11/30\n",
            "55/55 [==============================] - 7s 134ms/step - loss: 0.4937 - categorical_accuracy: 0.9685 - val_loss: 0.6938 - val_categorical_accuracy: 0.9084\n",
            "Epoch 12/30\n",
            "55/55 [==============================] - 7s 136ms/step - loss: 0.4516 - categorical_accuracy: 0.9745 - val_loss: 0.6668 - val_categorical_accuracy: 0.9129\n",
            "Epoch 13/30\n",
            "55/55 [==============================] - 8s 138ms/step - loss: 0.4321 - categorical_accuracy: 0.9718 - val_loss: 0.6634 - val_categorical_accuracy: 0.9084\n",
            "Epoch 14/30\n",
            "55/55 [==============================] - 7s 135ms/step - loss: 0.3968 - categorical_accuracy: 0.9786 - val_loss: 0.6175 - val_categorical_accuracy: 0.9072\n",
            "Epoch 15/30\n",
            "55/55 [==============================] - 8s 137ms/step - loss: 0.3671 - categorical_accuracy: 0.9807 - val_loss: 0.6314 - val_categorical_accuracy: 0.9107\n",
            "Epoch 16/30\n",
            "55/55 [==============================] - 8s 137ms/step - loss: 0.3701 - categorical_accuracy: 0.9735 - val_loss: 0.5854 - val_categorical_accuracy: 0.9198\n",
            "Epoch 17/30\n",
            "55/55 [==============================] - 8s 136ms/step - loss: 0.3214 - categorical_accuracy: 0.9858 - val_loss: 0.5430 - val_categorical_accuracy: 0.9233\n",
            "Epoch 18/30\n",
            "55/55 [==============================] - 7s 135ms/step - loss: 0.2933 - categorical_accuracy: 0.9896 - val_loss: 0.5425 - val_categorical_accuracy: 0.9164\n",
            "Epoch 19/30\n",
            "55/55 [==============================] - 7s 136ms/step - loss: 0.2839 - categorical_accuracy: 0.9891 - val_loss: 0.5566 - val_categorical_accuracy: 0.9038\n",
            "Epoch 20/30\n",
            "55/55 [==============================] - 8s 137ms/step - loss: 0.2735 - categorical_accuracy: 0.9856 - val_loss: 0.5201 - val_categorical_accuracy: 0.9198\n",
            "Epoch 21/30\n",
            "55/55 [==============================] - 7s 137ms/step - loss: 0.2543 - categorical_accuracy: 0.9898 - val_loss: 0.4711 - val_categorical_accuracy: 0.9233\n",
            "Epoch 22/30\n",
            "55/55 [==============================] - 8s 137ms/step - loss: 0.2342 - categorical_accuracy: 0.9930 - val_loss: 0.4926 - val_categorical_accuracy: 0.9210\n",
            "\n",
            "Epoch 00022: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
            "Epoch 23/30\n",
            "55/55 [==============================] - 7s 136ms/step - loss: 0.2152 - categorical_accuracy: 0.9952 - val_loss: 0.4573 - val_categorical_accuracy: 0.9255\n",
            "Epoch 24/30\n",
            "55/55 [==============================] - 7s 137ms/step - loss: 0.2007 - categorical_accuracy: 0.9976 - val_loss: 0.4332 - val_categorical_accuracy: 0.9393\n",
            "Epoch 25/30\n",
            "55/55 [==============================] - 7s 136ms/step - loss: 0.1942 - categorical_accuracy: 0.9973 - val_loss: 0.4257 - val_categorical_accuracy: 0.9359\n",
            "Epoch 26/30\n",
            "55/55 [==============================] - 8s 139ms/step - loss: 0.1862 - categorical_accuracy: 0.9978 - val_loss: 0.4350 - val_categorical_accuracy: 0.9336\n",
            "Epoch 27/30\n",
            "55/55 [==============================] - 7s 136ms/step - loss: 0.1838 - categorical_accuracy: 0.9974 - val_loss: 0.4356 - val_categorical_accuracy: 0.9267\n",
            "Epoch 28/30\n",
            "55/55 [==============================] - 8s 137ms/step - loss: 0.1822 - categorical_accuracy: 0.9964 - val_loss: 0.4140 - val_categorical_accuracy: 0.9290\n",
            "Epoch 29/30\n",
            "55/55 [==============================] - 8s 138ms/step - loss: 0.1774 - categorical_accuracy: 0.9972 - val_loss: 0.4104 - val_categorical_accuracy: 0.9313\n",
            "\n",
            "Epoch 00029: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
            "Epoch 30/30\n",
            "55/55 [==============================] - 7s 136ms/step - loss: 0.1699 - categorical_accuracy: 0.9988 - val_loss: 0.4002 - val_categorical_accuracy: 0.9370\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nM0VTc4lMvwA"
      },
      "source": [
        "We are now getting very high validation accuracy with our model at 93.70%. The training accuracy is now at a peak of 99.88%. It can be said that the max pooling and final fully connected layer are successful at feature distinction between label IDs."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Log6GkFMOCnw"
      },
      "source": [
        "One thing that should be noted is that we didn't use batch normalization after each convolution layer. Batch normalization is most commonly used to decrease the covariance shift of the output between convolutions to make the model converge faster. However, since the model begin converging very fast at 9 epochs, this doesn't seem necessary."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DZwaznxMw-R2"
      },
      "source": [
        "### 8. Inference"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O51woVBBNEix"
      },
      "source": [
        "Finally, we want to see how well the model performs on the test dataset that we created."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RUxs0hFPGQyZ"
      },
      "source": [
        "#### Testing Set Accuracy"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JNy-bJafycX7"
      },
      "source": [
        "# Testing out the model on the test set.\n",
        "test_pred = model2.predict(test)\n",
        "accuracy = tf.keras.metrics.CategoricalAccuracy()\n",
        "accuracy.update_state(test_label, test_pred)\n",
        "accuracy = accuracy.result().numpy()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B8pux1cXPxag",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fbcae47e-4f89-4f11-dbe0-ed8137cda878"
      },
      "source": [
        "# Printing the test accuracy:\n",
        "accuracy"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.946039"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 94
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3Hla427LNNVh"
      },
      "source": [
        "The test dataset has produced 94.60% accuracy."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v9u_gcMv1Cn_"
      },
      "source": [
        "#### Confusion Matrix"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "quThZVVPNSxc"
      },
      "source": [
        "It would also very interesting to see how different classes in the test dataset performed."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hSUic-jhHGDr"
      },
      "source": [
        "test_label_decoded = tf.math.argmax(test_label, axis=1)\n",
        "test_pred_decoded = tf.math.argmax(test_pred, axis=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fuLf3vRhO_o0"
      },
      "source": [
        "# Creating the numbers for the confusion matrix.\n",
        "confusion_matrix = tf.math.confusion_matrix(test_label_decoded, test_pred_decoded)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IEJZmqnjHqhG",
        "outputId": "330448eb-ee75-4505-bbf9-e2577f6ea235"
      },
      "source": [
        "confusion_matrix"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(10, 10), dtype=int32, numpy=\n",
              "array([[97,  0,  0,  0,  1,  0,  0,  0,  0,  2],\n",
              "       [ 0, 40,  0,  0,  1,  0,  0,  0,  0,  1],\n",
              "       [ 1,  0, 92,  2,  1,  1,  1,  0,  2,  0],\n",
              "       [ 1,  0,  2, 91,  1,  0,  1,  0,  3,  1],\n",
              "       [ 0,  0,  1,  0, 93,  0,  0,  2,  1,  3],\n",
              "       [ 0,  0,  1,  0,  0, 97,  0,  0,  1,  1],\n",
              "       [ 0,  0,  2,  1,  0,  0, 34,  0,  0,  0],\n",
              "       [ 0,  0,  1,  0,  0,  0,  0, 99,  0,  0],\n",
              "       [ 1,  0,  0,  2,  1,  0,  0,  0, 88,  0],\n",
              "       [ 1,  2,  2,  0,  0,  2,  0,  0,  0, 93]], dtype=int32)>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 104
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4A4obHUj1b4k",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 459
        },
        "outputId": "622c2892-2104-40c4-da2d-cb7f5cf97b53"
      },
      "source": [
        "# Printing the confusion matrix.\n",
        "df_cm = pd.DataFrame(np.array(confusion_matrix), index = [i for i in range(10)],\n",
        "                                       columns = [i for i in range(10)])\n",
        "plt.figure(figsize = (10,7))\n",
        "heatmap = sn.heatmap(df_cm, annot=True)\n",
        "heatmap.set(xlabel='Predictions', ylabel='Ground Truth Label')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Text(69.0, 0.5, 'Ground Truth Label'), Text(0.5, 42.0, 'Predictions')]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 113
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 720x504 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j6_3uI44NhqW"
      },
      "source": [
        "It can be said that the CNN model has performed very well across all classes. One detail that should be noted is that classes 1 and 6 seemed to underperform a little bit relative to the other classes. This is likely due to the dataset imbalance that was present in the dataset in the beginning."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "evN_xb0PJzHs"
      },
      "source": [
        "### 8. Conclusion"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EJJs9MAhfCQo"
      },
      "source": [
        "This concludes this project. It is fair to say that we have succeeded at producing a model that can accurately classify sounds."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CmxYqcDWQjlZ"
      },
      "source": [
        "If given more time and hardware resources, we can experiment more with how batch size, batch normalization, and different optimizers affect model performance on the validation and testing set. It would also be insightful to experiment more with model architecture and different types of models for time, such as LSTMs and Feed Forward Neural Networks. Lastly, I am interested in training the model for more epochs to see how the model behaves the more it trains."
      ]
    }
  ]
}